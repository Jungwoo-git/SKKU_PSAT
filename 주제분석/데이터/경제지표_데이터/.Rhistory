rf_y_pred2 <- ifelse(rf_y_pred<0.5, 0, 1)
fold_i_accu[i] <- Accuracy(rf_y_pred, rf_fit_test$OC)
}
mtry_accu[which(k == acc_rf)] <- mean(fold_i_accu)
}
acc_rf$acc <- mtry_accu
acc_rf
max_accu_index <- which(acc_rf$acc == acc_rf$acc %>% max)
acc_rf[max_accu_index, ]
rf_best <- randomForest(as.factor(OC) ~ revenue1 + salescost1 + sga1 + noi1 + noe1 + interest1 + ctax1 + profit1 + liquidAsset1 +
quickAsset1 + receivableS1 + nonCAsset1 + tanAsset1 + OnonCAsset1 + debt1 + surplus1 + ownerChange,
data = train_set,
mtry = acc_rf[max_accu_index, "mtry"],
ntree = 10)
rf_impo <- cbind(rf_best$importance %>% as.data.frame,
rownames=rf_best$importance %>% rownames)
ggplot(rf_impo,
aes(x = reorder(rownames, MeanDecreaseGini),
y = MeanDecreaseGini)) +
geom_point(color = "pink") +
geom_segment(aes(x = rownames,
y = 0, xend = rownames,
yend = MeanDecreaseGini,
color = 'pink')) +
xlab("Variable Name") +
coord_flip() +
theme_classic() +
theme(legend.position = 'none')
Bos_test_index <- createDataPartition(Boston$medv, p = 0.2, list = FALSE)
Bos_test_set <- Boston[Bos_test_index, ]
Bos_train_set <- Boston[-Bos_test_index, ]
RMSE_rf <- expand.grid(mtry = c(3,4,5),
ntree = c(10,100,200),
RMSE = NA)
RMSE_rf
Bos_fold <- createFolds(Bos_train_set$medv, 5, list = TRUE, returnTrain = FALSE)
save_bos_RMSE <- 1:5
save_bos_mean <- 1:nrow(RMSE_rf)
for (k in 1:nrow(RMSE_rf)) {
for (i in 1:5) {
Bos_rf_train <- Bos_train_set[-Bos_fold[[i]], ]
Bos_rf_test <- Bos_train_set[Bos_fold[[i]], ]
Bos_rf <- randomForest(medv~.,
data = Bos_rf_train,
mtry = RMSE_rf[k, "mtry"],
ntree = RMSE_rf[k, "ntree"])
Bos_rf_pred <- predict(Bos_rf, newdata =  Bos_rf_test) %>% as.data.frame
save_bos_RMSE[i] <- RMSE(Bos_rf_test$medv, Bos_rf_pred %>% as.matrix)
}
save_bos_mean[k] <- save_bos_RMSE %>% mean
}
RMSE_rf[, "RMSE"] <- save_bos_mean
RMSE_rf
RMSE_rf[which(RMSE_rf$RMSE ==( RMSE_rf$RMSE %>% min)), ]
Bos_rf_best <- randomForest(medv~ .,
data = Bos_train_set,
mtry = filter(RMSE_rf, RMSE ==RMSE_rf$RMSE %>% min)[["mtry"]],
ntree = filter(RMSE_rf, RMSE ==RMSE_rf$RMSE %>% min)[["ntree"]])
Bos_best_pred <- predict(Bos_rf_best, newdata = Bos_test_set)
final_RMSE <- RMSE(Bos_test_set$medv, Bos_best_pred)
final_RMSE
#단계선택법을 통해 변수 선택.
Bos_lm <- lm(medv~ .,
data = Bos_train_set)
Bos_selection <- step(Bos_lm, direction = 'both')
# randomForest 적용
RMSE_rf2 <- expand.grid(mtry = c(3,4,5),
ntree = c(10,100,200),
RMSE = NA)
Bos_fold2 <- createFolds(1: Bos_train_set$medv, 5, list = TRUE, returnTrain = FALSE)
save_bos_RMSE <- 1:5
save_bos_mean <- 1:nrow(RMSE_rf2)
for (k in 1:nrow(RMSE_rf2)) {
for (i in 1:5) {
Bos_rf_train <- Bos_train_set[-Bos_fold2[[i]], ]
Bos_rf_test <- Bos_train_set[Bos_fold2[[i]], ]
Bos_rf <- randomForest(medv~crim + zn + chas + nox + rm + dis + rad + tax + ptratio + black + lstat,
data = Bos_rf_train,
mtry = RMSE_rf[k, "mtry"], ntree = RMSE_rf[k, "ntree"])
Bos_rf_pred <- predict(Bos_rf, newdata =  Bos_rf_test) %>% as.data.frame
save_bos_RMSE[i] <- RMSE(Bos_rf_test$medv, Bos_rf_pred %>% as.matrix)
}
save_bos_mean[k] <- save_bos_RMSE %>% mean
}
RMSE_rf2[, "RMSE"] <- save_bos_mean
RMSE_rf2
RMSE_rf2[which(RMSE_rf2$RMSE ==( RMSE_rf2$RMSE %>% min)), ]
Bos_rf_best2 <- randomForest(medv~crim + zn + chas + nox + rm + dis + rad + tax + ptratio + black + lstat,
data = Bos_train_set,
mtry = filter(RMSE_rf2, RMSE ==RMSE_rf2$RMSE %>% min)[["mtry"]],
ntree = filter(RMSE_rf2, RMSE ==RMSE_rf2$RMSE %>% min)[["ntree"]])
Bos_best_pred <- predict(Bos_rf_best2, newdata = Bos_test_set)
final_RMSE2 <- RMSE(Bos_test_set$medv, Bos_best_pred)
final_RMSE2
knitr::opts_chunk$set(echo = TRUE, rood.dir = "C:/Users/10sop/Desktop/package/second_week",RNGkind(sample.kind = "Rounding"))
set.seed(1234)
library("tidyverse")
library("data.table")
library("VIM")
library("caret")
library("MLmetrics")
library("randomForest")
library("MASS")
setwd("C:/Users/10sop/Desktop/package/second_week")
fake_data <- 'data.csv' %>% fread()
last_number <- substr(fake_data %>% colnames,
fake_data %>% colnames %>% nchar,
fake_data %>% colnames %>% nchar)
col_name <- colnames(fake_data)[which(last_number !=2)]
data <- fake_data %>% subset(select= col_name)
data %>% colnames
label_colnames <- c(colnames(data)[1:11],
rep("",5),
colnames(data)[17],
rep("", 2),
colnames(data)[20:22],
"")
aggr(data,
numbers = TRUE,
prop = FALSE,
col = c('lightyellow', 'pink'),
cex.axis = .5,
labels = label_colnames)
numeric_data <- select_if(data, is.numeric) %>% as.data.frame
numeric_fix <- lapply(numeric_data,
function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))) %>% as.data.frame
data <- cbind(numeric_fix,
data[, !which((data %>% names) %in% (numeric_fix %>% names)),
with=FALSE])
classi_data <- select_if(data, is.character)
val <- 1
classi_data[, 1] %>% table %>% which.max %>% names
col_name <- classi_data %>% names
for (i in 1:length(classi_data)) {
val <- classi_data[, i] %>% table %>% which.max %>% names
classi_data[classi_data[, i] %>% is.na %>% which, i] <- val
data[, col_name[i]] <- classi_data[, col_name[i]]
}
data$OC <- gsub("open", 1, data$OC)
data$OC <- gsub("close", 0, data$OC)
data$OC <- data$OC %>% as.numeric
numeric_trans <- lapply(numeric_fix, as.numeric) %>% as.data.frame
data <- cbind(numeric_trans,
data[, -which((data %>% names) %in% (numeric_trans %>% names))])
set.seed(1234)
index_valid <- createDataPartition(data$OC , p = 0.3, list = FALSE)
valid_set <- data[index_valid, ]
train_set <- data[-index_valid, ]
head(valid_set)
logis <- glm(as.factor(OC)~.,
data = train_set,
family = binomial)
y_pred <- predict(logis,
newdata = valid_set,
type = 'response')
y_pred2 <- ifelse(y_pred<0.5, 0, 1)
Accuracy(y_pred2, valid_set$OC)
select_var <- step(logis, direction = "both")
new_logis <- glm(as.factor(OC) ~ revenue1 + salescost1 + sga1 +
noi1 + noe1 + interest1 + ctax1 + profit1 + liquidAsset1 +
quickAsset1 + receivableS1 + nonCAsset1 + tanAsset1 + OnonCAsset1 +
debt1 + surplus1 + ownerChange,
data = train_set,
family = binomial())
new_y_pred <- predict(new_logis,
newdata = valid_set,
type = 'response')
new_y_pred2 <- ifelse(new_y_pred<0.5, 0, 1)
## Accuracy 값 출력
Accuracy(new_y_pred2, valid_set$OC)
library("leaps")
cp <- regsubsets(as.factor(OC)~.,
data = train_set,
nvmax = train_set %>% length,
method = "exhaustive") %>% summary
cp$which[(cp$cp %>% min == cp$cp) %>% which, ]
cp_logis <- glm(as.factor(OC) ~ bedCount + sga1 + quickAsset1 + receivableS1 + receivableL1+ ownerChange,
data = train_set,
family = binomial())
cp_y_pred <- predict(cp_logis,
newdata = valid_set,
type= 'response')
cp_y_pred2 <- ifelse(cp_y_pred<0.5, 0, 1)
Accuracy(cp_y_pred2, valid_set$OC)
acc_rf <- expand.grid(mtry = c(3, 4, 5), acc = NA)
acc_rf
fold5 <- createFolds(train_set$OC, 5, list = TRUE, returnTrain = FALSE)
fold_i_accu <- 1:5
mtry_accu <- 1:3
for (k in acc_rf[, "mtry"]) {
for (i in 1:5) {
rf_fit_train <- train_set[-fold5[[i]], ]
rf_fit_test <- train_set[fold5[[i]], ]
rf_fit <- randomForest(OC~ revenue1 + salescost1 + sga1 +
noi1 + noe1 + interest1 + ctax1 + profit1 + liquidAsset1 +
quickAsset1 + receivableS1 + nonCAsset1 + tanAsset1 + OnonCAsset1 +
debt1 + surplus1 + ownerChange,
data = rf_fit_train, mtry = k, ntree = 10)
rf_y_pred <- predict(rf_fit, newdata = rf_fit_test)
rf_y_pred2 <- ifelse(rf_y_pred<0.5, 0, 1)
fold_i_accu[i] <- Accuracy(rf_y_pred, rf_fit_test$OC)
}
mtry_accu[which(k == acc_rf)] <- mean(fold_i_accu)
}
acc_rf$acc <- mtry_accu
acc_rf
max_accu_index <- which(acc_rf$acc == acc_rf$acc %>% max)
acc_rf[max_accu_index, ]
rf_best <- randomForest(as.factor(OC) ~ revenue1 + salescost1 + sga1 + noi1 + noe1 + interest1 + ctax1 + profit1 + liquidAsset1 +
quickAsset1 + receivableS1 + nonCAsset1 + tanAsset1 + OnonCAsset1 + debt1 + surplus1 + ownerChange,
data = train_set,
mtry = acc_rf[max_accu_index, "mtry"],
ntree = 10)
rf_impo <- cbind(rf_best$importance %>% as.data.frame,
rownames=rf_best$importance %>% rownames)
ggplot(rf_impo,
aes(x = reorder(rownames, MeanDecreaseGini),
y = MeanDecreaseGini)) +
geom_point(color = "pink") +
geom_segment(aes(x = rownames,
y = 0, xend = rownames,
yend = MeanDecreaseGini,
color = 'pink')) +
xlab("Variable Name") +
coord_flip() +
theme_classic() +
theme(legend.position = 'none')
Bos_test_index <- createDataPartition(Boston$medv, p = 0.2, list = FALSE)
Bos_test_set <- Boston[Bos_test_index, ]
Bos_train_set <- Boston[-Bos_test_index, ]
RMSE_rf <- expand.grid(mtry = c(3,4,5),
ntree = c(10,100,200),
RMSE = NA)
RMSE_rf
Bos_fold <- createFolds(Bos_train_set$medv, 5, list = TRUE, returnTrain = FALSE)
save_bos_RMSE <- 1:5
save_bos_mean <- 1:nrow(RMSE_rf)
for (k in 1:nrow(RMSE_rf)) {
for (i in 1:5) {
Bos_rf_train <- Bos_train_set[-Bos_fold[[i]], ]
Bos_rf_test <- Bos_train_set[Bos_fold[[i]], ]
Bos_rf <- randomForest(medv~.,
data = Bos_rf_train,
mtry = RMSE_rf[k, "mtry"],
ntree = RMSE_rf[k, "ntree"])
Bos_rf_pred <- predict(Bos_rf, newdata =  Bos_rf_test) %>% as.data.frame
save_bos_RMSE[i] <- RMSE(Bos_rf_test$medv, Bos_rf_pred %>% as.matrix)
}
save_bos_mean[k] <- save_bos_RMSE %>% mean
}
RMSE_rf[, "RMSE"] <- save_bos_mean
RMSE_rf
RMSE_rf[which(RMSE_rf$RMSE ==( RMSE_rf$RMSE %>% min)), ]
Bos_rf_best <- randomForest(medv~ .,
data = Bos_train_set,
mtry = filter(RMSE_rf, RMSE ==RMSE_rf$RMSE %>% min)[["mtry"]],
ntree = filter(RMSE_rf, RMSE ==RMSE_rf$RMSE %>% min)[["ntree"]])
Bos_best_pred <- predict(Bos_rf_best, newdata = Bos_test_set)
final_RMSE <- RMSE(Bos_test_set$medv, Bos_best_pred)
final_RMSE
#단계선택법을 통해 변수 선택.
Bos_lm <- lm(medv~ .,
data = Bos_train_set)
Bos_selection <- step(Bos_lm, direction = 'both')
# randomForest 적용
RMSE_rf2 <- expand.grid(mtry = c(3,4,5),
ntree = c(10,100,200),
RMSE = NA)
Bos_fold2 <- createFolds(Bos_train_set$medv, 5, list = TRUE, returnTrain = FALSE)
save_bos_RMSE <- 1:5
save_bos_mean <- 1:nrow(RMSE_rf2)
for (k in 1:nrow(RMSE_rf2)) {
for (i in 1:5) {
Bos_rf_train <- Bos_train_set[-Bos_fold2[[i]], ]
Bos_rf_test <- Bos_train_set[Bos_fold2[[i]], ]
Bos_rf <- randomForest(medv~crim + zn + chas + nox + rm + dis + rad + tax + ptratio + black + lstat,
data = Bos_rf_train,
mtry = RMSE_rf[k, "mtry"], ntree = RMSE_rf[k, "ntree"])
Bos_rf_pred <- predict(Bos_rf, newdata =  Bos_rf_test) %>% as.data.frame
save_bos_RMSE[i] <- RMSE(Bos_rf_test$medv, Bos_rf_pred %>% as.matrix)
}
save_bos_mean[k] <- save_bos_RMSE %>% mean
}
RMSE_rf2[, "RMSE"] <- save_bos_mean
RMSE_rf2
RMSE_rf2[which(RMSE_rf2$RMSE ==( RMSE_rf2$RMSE %>% min)), ]
Bos_rf_best2 <- randomForest(medv~crim + zn + chas + nox + rm + dis + rad + tax + ptratio + black + lstat,
data = Bos_train_set,
mtry = filter(RMSE_rf2, RMSE ==RMSE_rf2$RMSE %>% min)[["mtry"]],
ntree = filter(RMSE_rf2, RMSE ==RMSE_rf2$RMSE %>% min)[["ntree"]])
Bos_best_pred <- predict(Bos_rf_best2, newdata = Bos_test_set)
final_RMSE2 <- RMSE(Bos_test_set$medv, Bos_best_pred)
final_RMSE2
install.packages("ISLR")
v1 <- c(41, 46, 56, 56, 69)
v2 <- c(36, 7, 53, 31, 76)
v3 <- c(103, 75, 84, 82, 80)
v4 <- c(53, 30, 70, 80, 42)
sd(v1)
?sd
sd(v2)
sd(v3)
sd(v4)
sd(c(v1,v2,v3,v4))
data.frame(v1,v2,v3,v4)
data <- data.frame(v1,v2,v3,v4)
data
aov(data)
left_join(v1,v2,v3,v4)
library('dplyr')
left_join(v1,v2,v3,v4)
left_join(c(v1,v2,v3,v4))
vec <- c(v1,v2,v3,v4)
vec
vec <- cbind(c(v1,v2,v3,v4), c(rep('v1',4), rep('v2',4), rep('v3',4), rep('v4',v4)))
?rep
?reprep('v1',4)
rep('v1',4)
vec <- cbind(c(v1,v2,v3,v4), c(rep('v1',4), rep('v2',4), rep('v3',4), rep('v4',v4)))
c(rep('v1',4), rep('v2',4), rep('v3',4), rep('v4',v4))
c(rep('v1',4), rep('v2',4), rep('v3',4), rep('v4',v4))
c(rep('v1',4), rep('v2',4), rep('v3',4), rep('v4',4))
vec <- cbind(c(v1,v2,v3,v4), c(rep('v1',4), rep('v2',4), rep('v3',4), rep('v4',4)))
vec
vec[,1]
?%>%
library('tidyverse')
vec
vec[,1] <- vec[,1] %>% as.numeric
vec
vec[,1] <- vec[,1] %>% as.numeric
vec
vec[,1] %>% class
vec[,1]
vec[,1] %>% as.numeric
vec[,1<- vec[,1] %>% as.numeric
vec[,1<- vec[,1] %>% as.numeric
vec[,1]<- vec[,1] %>% as.numeric
vec[,1]<- vec[,1] %>% as.numeric
vec
vec[,1]
vecvec %>% as.data.frame
library('tidyverse')
vec <- vec %>% as.data.frame
vec
vec[,1]
vec[,1] %>% class
vec[,1] %>% as.integer
vec[,1] <- vec[,1] %>% as.integer
vec
vec[,1] %>% class
vec %>% names
aov(V1,V2, data = vec)
aov(V1~V2, data = vec)
vec
vec[,2] <- vec[,2] %>% as.factor
vec[,2] %>% class
aov(V1~V2, data = vec)
aov(V2~V1, data = vec)
aov(V2~V1, data = vec)
aov(V2~V1, data = vec) %>% summary
aov(V1~V2, data = vec) %>% summary
vec %>% names
vec$V1
aov(V1~V2, data = vec) %>% summary
vec
vec <- cbind(c(v1,v2,v3,v4), c(rep('v1',5), rep('v2',5), rep('v3',5), rep('v4',5)))
vec[,1] <- vec[,1] %>% as.integer
vec[,2] <- vec[,2] %>% as.factor
vec[,2] %>% class
vec[,2] %>% class
vec[,2] <- vec[,2] %>% as.factor
vec[,2] %>% class
vec
vec <- vec %>% as.data.frame()
vec
vec$V1 <- vec$V1 %>% as.integer
vec$V2 <- vec$V2 %>% as.factor
vec
aov(V1~V2, data = vec)
aov(V1~V2, data = vec) %>% summary
aov(V1~V2, data = vec)
aov(V1~V2, data = vec) %>% summary
aov(V1~V2, data = vec) %>% summary
v1 <- c(41, 46, 56, 56, 69)
v2 <- c(36, 7, 53, 31, 76)
v3 <- c(103, 75, 84, 82, 80)
v4 <- c(53, 30, 70, 80, 42)
data <- data.frame(v1,v2,v3,v4)
aov(, data= data)
vec
vec <- cbind(c(v1,v2,v3,v4), c(rep('v1',5), rep('v2',5), rep('v3',5), rep('v4',5)))
rep('v1',4)
vec <- vec %>% as.data.frame()
vec$V1 <- vec$V1 %>% as.integer
vec$V2 <- vec$V2 %>% as.factor
vec
vec$V2 <- names("treatment")
library('tidyverse')
vec %>% names
vec %>% names <- c("value", "treatment")
names(vec) <- c("value", "treatment")
vec
vec %>% names
vec <- cbind(c(v1,v2,v3,v4), c(rep('v1',5), rep('v2',5), rep('v3',5), rep('v4',5)))
rep('v1',4)
vec <- vec %>% as.data.frame()
vec$V1 <- vec$V1 %>% as.integer
vec$V2 <- vec$V2 %>% as.factor
vec %>% names
names(vec) <- c("value", "treatment")
vec %>% names
aov(V1~V2, data = vec) %>% summary
aov(value~treatment, data = vec) %>% summary
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("data.table")
library("gridExtra")
library("catboost")
library("caret")
library("MLmetrics")
library("factoextra")
library("cluster")
setwd("C:/Users/10sop/Desktop/package/third_week")
data <- "data.csv" %>% fread
testset <- "test.csv" %>% fread
set .seed (1)
x1=runif (100)
set.seed (1)
x2 =0.5* x1+rnorm (100) /10
x1
x2
y=2+2* x1 +0.3* x2+rnorm (100)
y
plot(x1, x2)
lm(y~x1+x2)
lm(y~x1+x2) %>% summary
library("tdplyr")
library("dplyr")
lm(y~x1+x2) %>% summary
cor(x1,x2)
plot(x1, x2)
cor(x1, x2)
lm(y~x1)
lm(y~x1) %>% summary
lm(y~x2) %>% summary
x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)
lm(y~x1+x2) %>% summary
lm(y~x1)
lm(y~x1) %>% summary
lm(y~x2) %>% summary
lm(y~x1+x2) %>% summary
lm(y~x1) %>% summary
lm(y~x2) %>% summary
x1
x2
lm(y~x1+x2) %>% summary
lm(y~x1+x2) %>% summary
lm(y~x1) %>% summary
lm(y~x2) %>% summary
lm(y~x1) %>% summary
install.packages("car")
library("car")
lm(y~x1+x2) %>% outlierTest()
lm(y~x1+x2) %>% outlierTest() [1]
outlierTest(lm(y~x1+x2))[1]
outlierTest(lm(y~x1+x2))
outlierTest(lm(y~x1+x2)) %>% summary
outlierTest(lm(y~x1+x2))[1, ]
outlierTest(lm(y~x1+x2))
outlierTest(lm(y~x1+x2))
outlierTest(lm(y~x1+x2)) %>% summary
outlierTest(lm(y~x1+x2))
library("car")
lm(y~x1+x2) %>% outlierTest()
lm(y~x1+x2) %>% leverage()
install.packages("sur")
library("sur")
library("sur")
lm(y~x1+x2) %>% leverage()
lm(y~x1+x2) %>% leverage() > 2*(2+1)/length(x1)
which(lm(y~x1+x2) %>% leverage() > 2*(2+1)/length(x1))
library("sur")
which(lm(y~x1+x2) %>% leverage() > 2*(2+1)/length(x1))
library("sur")
which(lm(y~x1+x2) %>% leverage() > 2*(2+1)/length(x1))
setwd("C:/Users/10sop/Desktop/통계분석학회 P-SAT/주제분석/데이터/경제지표 데이터")
exchange_rate <- "USD_KRW Historical Data" %>% fread
## library
library("tidyverse")
library("data.table")
library("gridExtra")
library("catboost")
library("caret")
library("MLmetrics")
## data
setwd("C:/Users/10sop/Desktop/통계분석학회 P-SAT/주제분석/데이터/경제지표 데이터")
exchange_rate <- "USD_KRW Historical Data" %>% fread
exchange_rate
exchange_rate <- "USD_KRW_exchange_rate" %>% fread
## data
setwd("C:/Users/10sop/Desktop/통계분석학회 P-SAT/주제분석/데이터/경제지표 데이터")
exchange_rate <- "USD_KRW_exchange_rate" %>% fread
## data
setwd("C:/Users/10sop/Desktop/통계분석학회 P-SAT/주제분석/데이터/경제지표 데이터")
getwd()
## data
setwd("C:/Users/10sop/Desktop/통계분석학회 P-SAT/주제분석/데이터/경제지표 데이터")
exchange_rate <- "USD_KRW_exchange_rate" %>% fread
exchange_rate <- "USD_KRW_exchange_rate" %>% fread
